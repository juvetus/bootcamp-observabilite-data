# ğŸš€ Databricks Spark Job - Docker

## âŒ ProblÃ¨me Windows

PySpark ne fonctionne pas nativement sur Windows car il utilise des composants Unix.

## âœ… Solution : Docker

### Lancer le job Spark

```bash
# Construire l'image
docker-compose build

# ExÃ©cuter le job
docker-compose up

# Voir les rÃ©sultats
ls output/truck_metrics/
```

### Alternative : WSL

Si vous prÃ©fÃ©rez utiliser WSL :

```bash
# Dans WSL (Ubuntu)
cd /mnt/c/DevOps/Bootcamp-Observabilite/databricks

# Installer les dÃ©pendances
pip install -r requirements.txt

# Lancer le job
python3 scripts/spark_job.py
```

## ğŸ“Š RÃ©sultats

Les fichiers Parquet seront gÃ©nÃ©rÃ©s dans `output/truck_metrics/`
